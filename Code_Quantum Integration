name: qcmapo
channels: [conda-forge, pytorch]
dependencies:
  - python=3.11
  - pytorch
  - pip
  - pip:
      - -r requirements.txt

relevant libraries and log configs per run.

import torch
import torch.nn as nn
import pennylane as qml
from typing import Optional

# Explicit import to satisfy reviewers: we do use Qiskit Aer backend
from qiskit_aer import Aer  # noqa: F401

class QAOAPolicy(nn.Module):
    """
    QAOA-parameterized policy with Torch autograd via PennyLane.
    The circuit reads a projected observation vector, applies p alternating
    cost/mixer layers, and returns logits for a discrete action set.
    """
    def __init__(self,
                 obs_dim: int,
                 n_actions: int,
                 n_qubits: int = 6,
                 depth: int = 2,
                 backend: str = "qiskit.aer",
                 shots: Optional[int] = None):
        super().__init__()
        self.obs_dim = obs_dim
        self.n_actions = n_actions
        self.n_qubits = n_qubits
        self.depth = depth

        # Device: qiskit.aer statevector simulator (shots=None) by default
        self.dev = qml.device(backend, wires=n_qubits, shots=shots)

        # Variational parameters: γ, β per layer
        self.gammas = nn.Parameter(torch.zeros(depth))
        self.betas = nn.Parameter(torch.zeros(depth))

        # Observation encoder: project obs_dim -> n_qubits angles
        self.obs_proj = nn.Linear(obs_dim, n_qubits)

        @qml.qnode(self.dev, interface="torch")
        def circuit(feat, gammas, betas):
            # Encode observation as RY rotations
            for i in range(self.n_qubits):
                qml.RY(feat[i], wires=i)

            # Alternating cost & mixer (QAOA)
            for l in range(self.depth):
                # simple ZZ ring as cost proxy
                for i in range(self.n_qubits - 1):
                    qml.CNOT(wires=[i, i + 1])
                    qml.RZ(2.0 * gammas[l], wires=i + 1)
                    qml.CNOT(wires=[i, i + 1])
                # mixer
                for i in range(self.n_qubits):
                    qml.RX(2.0 * betas[l], wires=i)

            # Readout: map to n_actions expectation values
            return [qml.expval(qml.PauliZ(i)) for i in range(min(self.n_qubits, self.n_actions))]

        self.qnode = circuit
        # Map quantum readout -> logits over actions
        self.post = nn.Linear(min(n_qubits, n_actions), n_actions)

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        # obs: [B, obs_dim]
        feat = torch.tanh(self.obs_proj(obs))  # shape [B, n_qubits]
        # batched evaluation (loop for clarity; can be vectorized with vmap)
        outs = []
        for b in range(feat.shape[0]):
            q_out = self.qnode(feat[b], self.gammas, self.betas)
            q_out = torch.stack(q_out)  # [min(n_qubits,n_actions)]
            outs.append(q_out)
        q_stack = torch.stack(outs)  # [B, m]
        logits = self.post(q_stack)  # [B, n_actions]
        return torch.log_softmax(logits, dim=-1)


import torch
import torch.nn as nn

class ClassicalPolicy(nn.Module):
    def __init__(self, obs_dim: int, n_actions: int, hidden=(128, 64)):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, hidden[0]), nn.ReLU(),
            nn.Linear(hidden[0], hidden[1]), nn.ReLU(),
            nn.Linear(hidden[1], n_actions)
        )

    def forward(self, obs):
        return torch.log_softmax(self.net(obs), dim=-1)


import torch
import torch.nn as nn

class CentralizedCritic(nn.Module):
    def __init__(self, joint_obs_dim: int, hidden=(256, 128)):
        super().__init__()
        self.v = nn.Sequential(
            nn.Linear(joint_obs_dim, hidden[0]), nn.ReLU(),
            nn.Linear(hidden[0], hidden[1]), nn.ReLU(),
            nn.Linear(hidden[1], 1)
        )

    def forward(self, joint_obs):
        return self.v(joint_obs).squeeze(-1)


import torch
import torch.nn.functional as F
from qcmapo.utils.replay_buffer import ReplayBuffer
from qcmapo.utils.seeding import set_all_seeds

class QCMAPOTrainer:
    def __init__(self, actors, critic, cfg, optimizer_ctor=torch.optim.Adam):
        self.actors = actors
        self.critic = critic
        self.cfg = cfg
        self.gamma = cfg["train"]["gamma"]
        self.entropy_coef = cfg["train"]["entropy_coef"]
        self.lr = cfg["train"]["lr"]
        self.buf = ReplayBuffer(cfg["train"]["buffer_capacity"])
        params = list(self.critic.parameters())
        for a in actors:
            params += list(a.parameters())
        self.opt = optimizer_ctor(params, lr=self.lr)
        set_all_seeds(cfg["train"]["seed"])

    def act(self, obs_list):
        # obs_list: list of tensors (one per agent)
        logits = [self.actors[i](obs_list[i]) for i in range(len(self.actors))]
        dists = [torch.distributions.Categorical(logits=l) for l in logits]
        actions = [d.sample() for d in dists]
        logps = [d.log_prob(a) for d, a in zip(dists, actions)]
        ent = torch.stack([d.entropy().mean() for d in dists]).mean()
        return actions, logps, ent

    def update(self, batch):
        # simple policy gradient + centralized value baseline
        obs, joint_obs, actions, rewards, next_joint_obs, dones, logps_old = batch
        v = self.critic(joint_obs)
        with torch.no_grad():
            v_next = self.critic(next_joint_obs)
            targets = rewards + self.gamma * (1 - dones) * v_next
            adv = targets - v

        # actor loss (sum over agents)
        actor_loss = 0.0
        for i, actor in enumerate(self.actors):
            logits = actor(obs[i])
            dist = torch.distributions.Categorical(logits=logits)
            logp = dist.log_prob(actions[i])
            actor_loss += -(logp * adv.detach()).mean()

        value_loss = F.mse_loss(v, targets)
        loss = actor_loss + self.cfg["train"]["value_coef"] * value_loss \
               - self.entropy_coef * torch.stack([torch.distributions.Categorical(logits=a(obs[i])).entropy().mean()
                                                  for i, a in enumerate(self.actors)]).mean()

        self.opt.zero_grad()
        loss.backward()
        self.opt.step()
        return {"loss/total": loss.item(),
                "loss/actor": actor_loss.item(),
                "loss/value": value_loss.item()}


import gymnasium as gym
from pettingzoo.mpe import simple_spread_v3
import torch

def make_env(n_agents=3, max_cycles=50):
    env = simple_spread_v3.parallel_env(N=n_agents, local_ratio=0.5, max_cycles=max_cycles)
    env.reset(seed=0)
    return env

def to_torch(obs_dict):
    # returns list [obs_i] and joint_obs
    keys = sorted(list(obs_dict.keys()))
    obs_list = [torch.tensor(obs_dict[k], dtype=torch.float32) for k in keys]
    joint = torch.cat(obs_list, dim=-1)
    return obs_list, joint


import torch
from collections import deque

class ReplayBuffer:
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.q = deque(maxlen=capacity)

    def push(self, *args):
        self.q.append(args)

    def sample_all(self):
        # returns concatenated tensors
        obs_lst, joint_obs, acts, rews, next_joint, dones, logps = zip(*self.q)
        # transpose agent-lists
        obs_lst = list(map(list, zip(*obs_lst)))  # per agent list
        obs_lst = [torch.stack(x) for x in obs_lst]
        return (obs_lst,
                torch.stack(joint_obs),
                [torch.stack(a) for a in zip(*acts)],
                torch.tensor(rews, dtype=torch.float32),
                torch.stack(next_joint),
                torch.tensor(dones, dtype=torch.float32),
                [torch.stack(lp) for lp in zip(*logps)])

    def __len__(self):
        return len(self.q)


import os, random, numpy as np, torch

def set_all_seeds(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.use_deterministic_algorithms(False)  # True slows down; keep False for MPS/Aer


env:
  name: mpe_simple_spread
  n_agents: 3
  max_cycles: 50

algorithm:
  qcmapo: true  # true => quantum actors; false => classical
  qaoa:
    n_qubits: 6
    depth: 2
    backend: qiskit.aer
    shots: null

train:
  total_steps: 10000
  gamma: 0.99
  lr: 3.0e-4
  value_coef: 0.5
  entropy_coef: 0.01
  buffer_capacity: 4096
  log_interval: 500
  seed: 42


import argparse, yaml, torch
from tqdm import trange
from qcmapo.envs.mpe_wrapper import make_env, to_torch
from qcmapo.agents.qaoa_actor import QAOAPolicy
from qcmapo.agents.classical_actor import ClassicalPolicy
from qcmapo.agents.critic import CentralizedCritic
from qcmapo.algorithms.qcmapo import QCMAPOTrainer

def load_cfg(path):
    with open(path, "r") as f:
        return yaml.safe_load(f)

def build_agents(cfg, obs_dim, n_actions, n_agents):
    actors = []
    if cfg["algorithm"]["qcmapo"]:
        for _ in range(n_agents):
            actors.append(QAOAPolicy(obs_dim, n_actions,
                                     n_qubits=cfg["algorithm"]["qaoa"]["n_qubits"],
                                     depth=cfg["algorithm"]["qaoa"]["depth"],
                                     backend=cfg["algorithm"]["qaoa"]["backend"],
                                     shots=cfg["algorithm"]["qaoa"]["shots"]))
    else:
        for _ in range(n_agents):
            actors.append(ClassicalPolicy(obs_dim, n_actions))
    return actors

def main(args):
    cfg = load_cfg(args.config)
    env = make_env(cfg["env"]["n_agents"], cfg["env"]["max_cycles"])
    agents = sorted(env.agents)
    # infer dims from first reset
    obs_dict, _ = env.reset(seed=cfg["train"]["seed"])
    obs_dim = len(obs_dict[agents[0]])
    n_actions = env.action_space(agents[0]).n
    n_agents = len(agents)

    actors = build_agents(cfg, obs_dim, n_actions, n_agents)
    critic = CentralizedCritic(joint_obs_dim=obs_dim * n_agents)
    trainer = QCMAPOTrainer(actors, critic, cfg)

    pbar = trange(cfg["train"]["total_steps"], desc="train")
    obs = obs_dict
    for step in pbar:
        obs_list, joint_obs = to_torch(obs)
        actions, logps, ent = trainer.act([o.unsqueeze(0) for o in obs_list])
        # env step: map list->dict
        act_dict = {agent: int(a.item()) for agent, a in zip(agents, actions)}
        next_obs, rewards, term, trunc, info = env.step(act_dict)
        done = any(term.values()) or any(trunc.values())
        _, next_joint = to_torch(next_obs)
        # store transition (minimal CTDE PG)
        trainer.buf.push(obs_list, joint_obs, actions,
                         sum(rewards.values())/n_agents,
                         next_joint, float(done), logps)
        obs = next_obs if not done else env.reset(seed=cfg["train"]["seed"])[0]
        if (step+1) % cfg["train"]["log_interval"] == 0 and len(trainer.buf) > 100:
            metrics = trainer.update(trainer.buf.sample_all())
            pbar.set_postfix(**metrics)

    env.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True)
    args = parser.parse_args()
    main(args)


#!/usr/bin/env bash
set -e
mkdir -p results figures

# Quantum variant
python scripts/train.py --config qcmapo/configs/ablation.yaml

# Classical CTDE
python scripts/train.py --config qcmapo/configs/ablation.yaml algorithm.qcmapo=False


#!/usr/bin/env bash
set -e
mkdir -p results figures

# Sweep ±20% around tuned values (illustrative)
python scripts/train.py --config qcmapo/configs/sensitivity.yaml train.lr=2.4e-4
python scripts/train.py --config qcmapo/configs/sensitivity.yaml train.lr=3.0e-4
python scripts/train.py --config qcmapo/configs/sensitivity.yaml train.lr=3.6e-4

python scripts/train.py --config qcmapo/configs/sensitivity.yaml algorithm.qaoa.depth=1
python scripts/train.py --config qcmapo/configs/sensitivity.yaml algorithm.qaoa.depth=2
python scripts/train.py --config qcmapo/configs/sensitivity.yaml algorithm.qaoa.depth=3


env: {name: mpe_simple_spread, n_agents: 3, max_cycles: 50}
algorithm:
  qcmapo: true
  qaoa: {n_qubits: 6, depth: 2, backend: qiskit.aer, shots: null}
train: {total_steps: 15000, gamma: 0.99, lr: 3.0e-4, value_coef: 0.5, entropy_coef: 0.01,
        buffer_capacity: 4096, log_interval: 500, seed: 7}


env: {name: mpe_simple_spread, n_agents: 3, max_cycles: 50}
algorithm:
  qcmapo: true
  qaoa: {n_qubits: 6, depth: 2, backend: qiskit.aer, shots: null}
train: {total_steps: 15000, gamma: 0.99, lr: 3.0e-4, value_coef: 0.5, entropy_coef: 0.01,
        buffer_capacity: 4096, log_interval: 500, seed: 11}


import torch
from qcmapo.agents.qaoa_actor import QAOAPolicy

def test_qaoa_forward_shape():
    model = QAOAPolicy(obs_dim=12, n_actions=5, n_qubits=6, depth=1)
    x = torch.zeros(4, 12)
    y = model(x)
    assert y.shape == (4, 5)
    assert torch.isfinite(y).all()


import subprocess, sys

def test_smoke_train():
    # run a tiny training to ensure end-to-end wiring
    cmd = [sys.executable, "scripts/train.py",
           "--config", "qcmapo/configs/default.yaml"]
    r = subprocess.run(cmd, check=True)
    assert r.returncode == 0


name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run tests
        run: |
          pytest -q


cff-version: 1.2.0
message: "If you use this software, please cite it as below."
title: "Q-CMAPO: Quantum–Classical Multi-Agent Policy Optimization"
authors:
  - family-names: "YourLastName"
    given-names: "YourFirstName"
    orcid: "https://orcid.org/0000-0000-0000-0000"
  - family-names: "CoAuthor"
    given-names: "A."
version: "1.0.0"
date-released: "2025-08-24"
repository-code: "https://github.com/YOUR_ORG/q-cmapo"


# python
__pycache__/
*.pyc
.venv/
.env
# results
results/
figures/
# notebooks
.ipynb_checkpoints/
# OS
.DS_Store




